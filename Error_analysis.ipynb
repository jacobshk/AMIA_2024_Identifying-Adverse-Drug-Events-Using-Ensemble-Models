{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f802d-c23b-42fb-9e81-d7fd24de0eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Error analysis module\n",
    "Input: the outputs of classification models on the dataframe. \n",
    "Specifically: result_dict = {\"predictions\": all_predictions, \"references\": all_references, \"filenames\": all_filenames, 'input_ids': all_input_ids} where:\n",
    "all_predictions is the binary predictions for the file, 0 or 1\n",
    "all_references is the binary references (ground truth) for the file, 0 or 1\n",
    "all_filenames is the name of the folder (discharge summary et.c)\n",
    "input ids is the tokenized, encoded versions of the original sentences (must decode here\n",
    "Functions:\n",
    "- error by folder: retrieve number of errors per folder \n",
    "- error length: retrieve average sentence length for correct, incorrect classifications:\n",
    "- error by words: retrieve most common words present in incorrect/correct sentences \n",
    "- error by model: for result_dict from each model, which errors are present in all models?\n",
    "- error by drug: given list of all drug strings, which drugs are most common in errors? \n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52735bd-ffbb-4912-b6d5-27bbc7b371dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def import_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        result_dict = pickle.load(f)\n",
    "    #print(result_dict['decoded_sentences'][0])\n",
    "        return(result_dict)\n",
    "\n",
    "\n",
    "def export_data(data, path):\n",
    "    f = open(path+\".pkl\",'w')\n",
    "    f.close()\n",
    "    f = open(path+\".pkl\", \"wb\")\n",
    "    pickle.dump(data, f)\n",
    "    f.close()\n",
    "    \n",
    "T5_result_dict = import_data('PT_T5_Classifier_results_dict_TEST_SET_v1.pkl')\n",
    "  \n",
    "BERT_result_dict = import_data('PT_BERT_Classifier_results_dict_TEST_SET_v1.pkl')\n",
    "\n",
    "SVM_result_dict = import_data('SVM_predictions.pkl')\n",
    "\n",
    "T5_int_list = [int(tensor.numpy()) for tensor in T5_result_dict['row_numbers']]\n",
    "print(\"biggest value in T5 (should match number of vals in test set\")\n",
    "print(max(T5_int_list))\n",
    "\n",
    "\n",
    "\n",
    "BERT_int_list = [int(tensor.numpy()) for tensor in BERT_result_dict['row_numbers']]\n",
    "print(\"biggest value in BERT (should match number of vals in test set\")\n",
    "print(max(BERT_int_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7ffb4-4e4e-4b9e-a29e-0fa3c41cf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_incorrect(results_dict):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\n",
    "        'sentence': results_dict[\"decoded_sentences\"],\n",
    "        'prediction': results_dict[\"predictions\"],\n",
    "        'reference': results_dict[\"references\"]\n",
    "    })\n",
    "    # Assuming you have a DataFrame named 'df' with columns 'sentence', 'prediction', and 'reference'\n",
    "    # Create a new DataFrame for correct and incorrect predictions\n",
    "    correct_df = pd.DataFrame(columns=['correct'])\n",
    "    incorrect_df = pd.DataFrame(columns=['incorrect'])\n",
    "\n",
    "    # Iterate through each row in the original DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        if row['prediction'] == row['reference']:\n",
    "            correct_df = correct_df.append({'correct': row['sentence']}, ignore_index=True)\n",
    "        else:\n",
    "            incorrect_df = incorrect_df.append({'incorrect': row['sentence']}, ignore_index=True)\n",
    "\n",
    "    return({\"correct\": correct_df, \"incorrect\":incorrect_df})\n",
    "\n",
    "\"\"\"\n",
    "result_dict = import_data('PT_T5_Classifier_results_dict.pkl')\n",
    "dict1 = get_correct_incorrect(result_dict)\n",
    "print(dct)\n",
    "# Specify the path and filename for the Excel file\n",
    "import csv\n",
    "with open('T5_corect_incorrect.csv', 'w') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for key, value in dict1.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "        \n",
    "result_dict = import_data('PT_BERT_Classifier_results_dict.pkl')\n",
    "dict1 = get_correct_incorrect(result_dict)\n",
    "print(dct)\n",
    "# Specify the path and filename for the Excel file\n",
    "with open('BERT_corect_incorrect.csv', 'w') as o:\n",
    "    writer = csv.writer(o, quoting=csv.QUOTE_NONNUMERIC)\n",
    "    for key, value in dict1.items():\n",
    "        writer.writerow([key, value])\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb2199-1a6c-4091-81d6-7ca89780b2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_by_folder(results_dict):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a DataFrame with filenames, predictions, and references\n",
    "    df = pd.DataFrame({\n",
    "        'filename': result_dict[\"filenames\"],\n",
    "        'prediction': result_dict[\"predictions\"],\n",
    "        'reference': result_dict[\"references\"]\n",
    "    })\n",
    "\n",
    "    # Identify correct and incorrect predictions\n",
    "    df['correct'] = (df['prediction'] == df['reference'])\n",
    "\n",
    "    # Group by filename and calculate total correct and incorrect predictions\n",
    "    summary_df = df.groupby('filename')['correct'].value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    summary_df.columns = ['filename', 'incorrect_predictions',  'correct_predictions']\n",
    "\n",
    "    return(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2470a-af43-43c6-8986-db9344da8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_by_length_char(result_dict):\n",
    "    #returns average length in characters of all incorrect sentence,  correct sentence\n",
    "    correct_sum = 0\n",
    "    correct_length = 0\n",
    "    \n",
    "    incorrect_sum = 0\n",
    "    incorrect_length = 0\n",
    "\n",
    "    \n",
    "    for i in range(len(result_dict['decoded_sentences'])):\n",
    "        #get correct sentences\n",
    "        if(result_dict['predictions'][i] == result_dict['references'][i]):\n",
    "            correct_length += 1\n",
    "            correct_sum += len(result_dict['decoded_sentences'][i])\n",
    "            \n",
    "        #Get incorrect\n",
    "        else:\n",
    "            incorrect_sum += len(result_dict['decoded_sentences'][i])\n",
    "            incorrect_length += 1\n",
    "            \n",
    "    average_correct_length_char = correct_sum / correct_length\n",
    "    average_incorrect_length_char = incorrect_sum / incorrect_length\n",
    "\n",
    "    return({\"average_incorrect_length_char\": average_incorrect_length_char, \"average_correct_length_char\":average_correct_length_char})\n",
    "        \n",
    "print(\"BERT\")\n",
    "print(error_by_length_char(BERT_result_dict))\n",
    "\n",
    "print(\"T5\")\n",
    "print(error_by_length_char(T5_result_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87963ac1-612d-4580-9ba3-75123c63c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def process_sentence(sentence, stopwords):\n",
    "    # Tokenize sentence, remove punctuation, and convert to lowercase\n",
    "    words = [word.strip(\".,?!\").lower() for word in sentence.split()]\n",
    "    # Remove stopwords and return the remaining words\n",
    "    return [word for word in words if word not in stopwords]\n",
    "\n",
    "def analyze_sentences(correct_sentences, incorrect_sentences, stopwords=[]):\n",
    "    correct_counts = Counter()\n",
    "    incorrect_counts = Counter()\n",
    "\n",
    "    # Process correct sentences\n",
    "    for sentence in correct_sentences:\n",
    "        sentence = ' '.join(sentence)\n",
    "        words = process_sentence(sentence, stopwords)\n",
    "        correct_counts.update(words)\n",
    "\n",
    "    # Process incorrect sentences\n",
    "    for sentence in incorrect_sentences:\n",
    "        sentence = ' '.join(sentence)\n",
    "        words = process_sentence(sentence, stopwords)\n",
    "        incorrect_counts.update(words)\n",
    "\n",
    "    # Return sorted results\n",
    "    return {\n",
    "        'correct': correct_counts.most_common(),\n",
    "        'incorrect': incorrect_counts.most_common()\n",
    "    }\n",
    "\n",
    "\n",
    "def get_common_words(result_dict):\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "    correctness_df = get_correct_incorrect(result_dict)\n",
    "    display(correctness_df['correct'])\n",
    "    display(correctness_df['incorrect'])\n",
    "\n",
    "\n",
    "    correct_sentences = correctness_df['correct'].values.tolist()\n",
    "    incorrect_sentences = correctness_df['incorrect'].values.tolist()\n",
    "    #print(correct_sentences[0])\n",
    "    stopwords = stopwords.words('english')\n",
    "    stopwords += (['-', 'ml', 'po', 'mg', 'pt'])\n",
    "    #stopwords = []\n",
    "\n",
    "\n",
    "    result = analyze_sentences(correct_sentences, incorrect_sentences, stopwords)\n",
    "    print(\"Most common words in correct sentences:\", result['correct'][0:31])\n",
    "    print(\"\\nMost common words in incorrect sentences:\", result['incorrect'][0:31])\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(\"BERT\")\n",
    "print(get_common_words(BERT_result_dict))\n",
    "\n",
    "print(\"T5\")\n",
    "print(get_common_words(T5_result_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79121ec-bf9e-45ca-95a0-a441ad9678fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_common_incorrect_words(T5_result_dict, BERT_result_dict):\n",
    "    T5_incorrect = get_correct_incorrect(T5_result_dict)['incorrect']\n",
    "    display(T5_incorrect)\n",
    "    print(type(T5_incorrect))\n",
    "    T5_incorrect.rename(columns={'incorrect': 'T5_incorrect'}, inplace=True)\n",
    "\n",
    "    BERT_incorrect = get_correct_incorrect(BERT_result_dict)['incorrect']\n",
    "    display(BERT_incorrect)\n",
    "    BERT_incorrect.rename(columns={'incorrect': 'BERT_incorrect'}, inplace=True)\n",
    "\n",
    "    print(type(BERT_incorrect))\n",
    "    \n",
    "    merged_df = pd.concat([T5_incorrect, BERT_incorrect], axis=1)\n",
    "    excel_file_path = 'actual_output.xlsx'\n",
    "    merged_df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "    \n",
    "get_common_incorrect_words(T5_result_dict, BERT_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdd52d-75b5-49ce-b38f-05ef39c8970b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#IMPROVEMENT FROM SVM\n",
    "#Get the sentences that were misclassified by SVM AND NOT (BERT, T5)\n",
    "SVM_only_wrong = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "def SVM_difference(SVM_result_dict, T5_result_dict, BERT_result_dict):\n",
    "    print(T5_result_dict.keys())\n",
    "    print(len(T5_result_dict['row_numbers']))\n",
    "    \n",
    "    print(SVM_result_dict.keys())\n",
    "    \n",
    "    \"\"\"\n",
    "    FALSE: T5_result_dict['row_numbers'][i] == BERT_result_dict['row_numbers'][i] == SVM_predictions[i] \n",
    "    \n",
    "    the lists did not keep their order; train_dataframe[i] does not equal BERT or SVM or T5[i]\n",
    "    to compare the same i: treat SVM_ground_truth[i] as i, treat T5/BERT_result_dict[row number] as i\n",
    "    \n",
    "            this method allows you to iterate over the same original element on all 3 dicts\n",
    "        T5_ground_truth = 0 \n",
    "        BERT_ground_truth =  0\n",
    "        SVM_ground_truth = SVM_result_dict['ground_truth'][i]\n",
    "        \n",
    "        for j in range(len((T5_result_dict['row_numbers']))):\n",
    "            if T5_result_dict['row_numbers'][j] == i:\n",
    "                #this means that T5[j] == SVM[i]\n",
    "                T5_ground_truth = T5_result_dict['references'][j]\n",
    "        for k in range(len((BERT_result_dict['row_numbers']))):\n",
    "            if BERT_result_dict['row_numbers'][k] == i:\n",
    "                #this means that BERT[k] == SVM[i]\n",
    "                BERT_ground_truth = BERT_result_dict['references'][k]\n",
    "        \n",
    "        if not (BERT_ground_truth == T5_ground_truth and BERT_ground_truth == SVM_ground_truth):\n",
    "            print(\"here\")\n",
    "    \"\"\"\n",
    "    #list of sentences that SVM got wrong, T5 AND BERT got right\n",
    "    for i in tqdm(range(len(SVM_result_dict['ground_truth']))):\n",
    "            T5_result = 0 \n",
    "            BERT_result =  0\n",
    "            SVM_result = SVM_result_dict['predictions'][i]\n",
    "            ground_truth = SVM_result_dict['ground_truth'][i]\n",
    "            sentence = ''\n",
    "            for j in range(len((T5_result_dict['row_numbers']))):\n",
    "                if T5_result_dict['row_numbers'][j] == i:\n",
    "                    sentence = T5_result_dict['decoded_sentences'][j]\n",
    "                    #this means that T5[j] == SVM[i]\n",
    "                    T5_result = T5_result_dict['predictions'][j]\n",
    "            for k in range(len((BERT_result_dict['row_numbers']))):\n",
    "                if BERT_result_dict['row_numbers'][k] == i:\n",
    "                    #this means that BERT[k] == SVM[i]\n",
    "                    BERT_result = BERT_result_dict['predictions'][k]\n",
    "            #If SVM got wrong\n",
    "            if not SVM_result == ground_truth:\n",
    "                #If T5 and BERT got right\n",
    "                if(T5_result == BERT_result and T5_result == ground_truth):\n",
    "                    SVM_only_wrong.append(sentence)\n",
    "                    \n",
    "    with open('errors_ONLY_SVM.txt', 'w') as f:\n",
    "        for line in SVM_only_wrong:\n",
    "            f.write(f\"{line}\\n\\n\")\n",
    "    return(SVM_only_wrong)\n",
    "        \n",
    "SVM_difference(SVM_result_dict, T5_result_dict, BERT_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fda446-63ec-4fa2-ab03-845f739e94d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#HARDEST SENTENCES\n",
    "#Get the sentences that were misclassified by SVM AND BERT AND T5\n",
    "SVM_correct = []\n",
    "SVM_wrong = []\n",
    "correct = []\n",
    "all_wrong = []\n",
    "from tqdm import tqdm\n",
    "def hard_sentences(SVM_predictions, T5_result_dict, BERT_result_dict):\n",
    "    print(T5_result_dict.keys())\n",
    "    print(len(T5_result_dict['row_numbers']))\n",
    "    \n",
    "    print(SVM_result_dict.keys())\n",
    "    \n",
    "    #list of sentences that SVM AND T5 AND BERT got wrong\n",
    "    wrong = []\n",
    "    for i in tqdm(range(len(SVM_result_dict['ground_truth']))):\n",
    "            T5_result = 0 \n",
    "            BERT_result =  0\n",
    "            SVM_result = SVM_result_dict['predictions'][i]\n",
    "            ground_truth = SVM_result_dict['ground_truth'][i]\n",
    "            sentence = ''\n",
    "            for j in range(len((T5_result_dict['row_numbers']))):\n",
    "                if T5_result_dict['row_numbers'][j] == i:\n",
    "                    sentence = T5_result_dict['decoded_sentences'][j]\n",
    "                    #this means that T5[j] == SVM[i]\n",
    "                    T5_result = T5_result_dict['predictions'][j]\n",
    "            for k in range(len((BERT_result_dict['row_numbers']))):\n",
    "                if BERT_result_dict['row_numbers'][k] == i:\n",
    "                    #this means that BERT[k] == SVM[i]\n",
    "                    BERT_result = BERT_result_dict['predictions'][k]\n",
    "            #If all got wrong\n",
    "            if not (SVM_result == ground_truth) and not (T5_result == BERT_result) and not (T5_result == ground_truth):\n",
    "                wrong.append(sentence)\n",
    "                all_wrong.append({'sentence':sentence, 'ground_truth':ground_truth})\n",
    "            if (SVM_result == ground_truth) and (T5_result == BERT_result) and (T5_result == ground_truth):\n",
    "                correct.append(sentence)\n",
    "            if (SVM_result == ground_truth):\n",
    "                SVM_correct.append(sentence)\n",
    "            if not (SVM_result == ground_truth):\n",
    "                SVM_wrong.append({'sentence':sentence, 'ground_truth':ground_truth})\n",
    "    with open('errors_all.txt', 'w') as f:\n",
    "        for line in wrong:\n",
    "            f.write(f\"{line}\\n\\n\")\n",
    "    return(wrong)\n",
    "        \n",
    "hard_sentences(SVM_result_dict, T5_result_dict, BERT_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19031ffc-3ae5-492e-87bd-6a3f3500903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get SVM misclass nature\n",
    "for i in SVM_wrong:\n",
    "    sentence = i['sentence']\n",
    "    #Get SVM misclass\n",
    "    if(sentence.split(' ')[0] == 'Past'):\n",
    "        if(sentence.split(' ')[1] == 'Medical/Surgical'):\n",
    "            print(sentence)\n",
    "            print(i['ground_truth'])\n",
    "\n",
    "all_false_positives = 0 \n",
    "all_false_negatives = 0\n",
    "for i in all_wrong:\n",
    "    sentence = i['sentence']\n",
    "    #correct answer was 0 (noADE), all got 1 \n",
    "    if(i['ground_truth'] == 0):\n",
    "        all_false_negatives +=1\n",
    "    if(i['ground_truth'] == 1):\n",
    "        all_false_positives +=1\n",
    "print('all false positives:', all_false_positives)\n",
    "print('all false negatives:', all_false_negatives)\n",
    "print('number of sentences incorrectly classified by all models:',len(all_wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd9650-be28-4042-a318-5040ff471c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Goal: determine how BERT/T5 improves on SVM. Show diff. between SVM correct/incorrect\n",
    "#get avg length of correct / incorrect sentence \n",
    "#get avg word length of correct / incorrect sentence \n",
    "\n",
    "SVM_wrong_count = 0\n",
    "for i in SVM_wrong:\n",
    "    SVM_wrong_count += len(i)\n",
    "print('avg sentence length (in chars) of incorrectly classified SVM sentence')\n",
    "print(SVM_wrong_count / len(SVM_wrong))\n",
    "\n",
    "\n",
    "SVM_correct_count = 0\n",
    "for i in SVM_correct:\n",
    "    SVM_correct_count += len(i)\n",
    "print('avg sentence length (in chars) of correctly classified SVM sentence')\n",
    "print(SVM_correct_count / len(SVM_correct))\n",
    "\n",
    "print(\"BERT\")\n",
    "print(error_by_length_char(BERT_result_dict))\n",
    "\n",
    "print(\"T5\")\n",
    "print(error_by_length_char(T5_result_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ccc67-9757-4c08-8f7d-40ff44cc9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('errors_all.txt', encoding='utf-8') as f:\n",
    "    length = 0\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        length += len(line)\n",
    "        count+=1\n",
    "    print(\"average length of incorrectly classified sentneces by all models\")\n",
    "    print(length / count)\n",
    "    print('number of incorrectly classified sentences by all models')\n",
    "    print(count)\n",
    "    \n",
    "avg_length_SVM_correct = 0\n",
    "\n",
    "length_correct = 0\n",
    "count_correct = 0\n",
    "for line in correct:\n",
    "    length_correct += len(line)\n",
    "    count_correct += 1\n",
    "    \n",
    "print('\\naverage length of correctly classified sentences by all models')\n",
    "print(length_correct / count_correct)\n",
    "print('number of correctly classified sentences by all models')\n",
    "print(len(correct))\n",
    "\n",
    "avg_word_length_SVM_correct = 0\n",
    "avg_word_length_SVM_incorrect = 0\n",
    "\n",
    "\n",
    "#print((SVM_correct[0]))\n",
    "for i in SVM_correct:\n",
    "    word_list = i.split()\n",
    "    if len(word_list) <= 0:\n",
    "        continue\n",
    "    total_avg = sum( map(len, word_list) ) / len(word_list)\n",
    "    avg_word_length_SVM_correct += total_avg \n",
    "    avg_word_length_SVM_correct /= 2\n",
    "    \n",
    "print('\\navg word length in correctly classified SVM sentences')\n",
    "print(avg_word_length_SVM_correct)\n",
    "print('number of correctly classified SVM sentences:')\n",
    "print(len(SVM_correct))\n",
    "\n",
    "for i in SVM_only_wrong:\n",
    "    word_list = i.split()     \n",
    "    total_avg = sum( map(len, word_list) ) / len(word_list)\n",
    "    avg_word_length_SVM_incorrect += total_avg \n",
    "    avg_word_length_SVM_incorrect /= 2\n",
    "\n",
    "print('\\navg word length in incorrectly classified SVM sentences')\n",
    "print(avg_word_length_SVM_incorrect)\n",
    "print('number of sentences only incorrectly classified by SVM')\n",
    "print(len(SVM_only_wrong))\n",
    "\n",
    "with open('errors_ONLY_SVM.txt', encoding='utf-8') as f:\n",
    "    length = 0\n",
    "    count = 0\n",
    "    for line in f:\n",
    "        length += len(line)\n",
    "        count+=1\n",
    "        \n",
    "    print(\"\\naverage character length of incorrectly classified SVM sentences\")\n",
    "    print(length / count)\n",
    "\n",
    "SVM_length = 0\n",
    "SVM_count = 0\n",
    "for i in SVM_correct:\n",
    "    SVM_count += 1\n",
    "    SVM_length += len(i)\n",
    "print(\"average character length of correctly classified sentence by SVM\")\n",
    "print(SVM_length / SVM_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f64e3d-284e-4b3d-8e1e-3de3adc0d613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "def get_uncommon_errors_between_models(model_outputs: list[dict]):\n",
    "    \n",
    "    correct_df = pd.DataFrame({'correct': []})\n",
    "    incorrect_df = pd.DataFrame({'incorrect': []})\n",
    "    counter = 0\n",
    "    for results_dict in model_outputs:\n",
    "        correctness_df = get_correct_incorrect(results_dict)\n",
    "        incorrect_sentences = correctness_df['incorrect']\n",
    "        temp2 = pd.DataFrame()\n",
    "\n",
    "        temp2['incorrect'] = incorrect_sentences['incorrect']\n",
    "        if(counter == 0):\n",
    "            incorrect_df = temp2\n",
    "        else:\n",
    "            incorrect_df = pd.merge(incorrect_df, temp2, on='incorrect', how='outer')\n",
    "        counter += 1\n",
    "\n",
    "    display(incorrect_df)\n",
    "    return(incorrect_df)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_most_common_errors_between_models(model_outputs: list[dict]):\n",
    "    \n",
    "    correct_df = pd.DataFrame({'correct': []})\n",
    "    incorrect_df = pd.DataFrame({'incorrect': []})\n",
    "    counter = 0\n",
    "    for results_dict in model_outputs:\n",
    "        correctness_df = get_correct_incorrect(results_dict)\n",
    "        incorrect_sentences = correctness_df['incorrect']\n",
    "        df = pd.DataFrame.from_dict([correctness_df])\n",
    "\n",
    "        df.to_excel(f'results1_{counter}.xlsx', index=False)\n",
    "\n",
    "        temp2 = pd.DataFrame()\n",
    "\n",
    "        temp2['incorrect'] = incorrect_sentences['incorrect']\n",
    "        if(counter == 0):\n",
    "            incorrect_df = temp2\n",
    "        else:\n",
    "            incorrect_df = pd.merge(incorrect_df, temp2, on='incorrect', how='inner')\n",
    "        counter += 1\n",
    "\n",
    "    display(incorrect_df)\n",
    "    return(incorrect_df)\n",
    "\n",
    "T5_result_dict = import_data('PT_T5_Classifier_results_dict.pkl')\n",
    "BERT_result_dict = import_data('PT_BERT_Classifier_results_dict.pkl')\n",
    "\n",
    "common_incorrect_df = get_most_common_errors_between_models([T5_result_dict, BERT_result_dict])\n",
    "uncommon_incorrect_df = get_uncommon_errors_between_models([T5_result_dict, BERT_result_dict])\n",
    "\n",
    "print(len(common_incorrect_df))\n",
    "print(len(uncommon_incorrect_df))\n",
    "\n",
    "#35export_data(incorrect_df, 'common_errors_BERT_T5')\n",
    "\n",
    "\n",
    "length = 0\n",
    "count = 0\n",
    "\"\"\"\n",
    "for i in common_incorrect_df['incorrect']:\n",
    "    print(i)\n",
    "    length += len(i)\n",
    "    #print(length)\n",
    "    print('\\n')\n",
    "    count += 1\n",
    "    \n",
    "print(\"avg length: \", length/count)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9546396-f8f6-4def-b385-1250a762c90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicate_strings = uncommon_incorrect_df[uncommon_incorrect_df.duplicated('incorrect')]\n",
    "\n",
    "# Display the rows with duplicate strings\n",
    "print(\"Rows with duplicate strings:\")\n",
    "print(duplicate_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ac5620-ef81-4fba-b906-5a72d36c50ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(get_correct_incorrect(T5_result_dict)['incorrect'])\n",
    "\n",
    "display(get_correct_incorrect(BERT_result_dict)['incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446c6bd-b56d-4289-8ef8-55163ffd1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "print(get_correct_incorrect(T5_result_dict)['incorrect'].iloc[4])\n",
    "print(get_correct_incorrect(BERT_result_dict)['incorrect'].iloc[1])\n",
    "\n",
    "#print(get_correct_incorrect(BERT_result_dict)['incorrect',1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf47974-c22c-41d9-b488-4c77989b49c3",
   "metadata": {},
   "source": [
    "Difference is due to tokenization -- we encode it, classify, then decode it to perform error analysis; when encoding, it seems like some characters are toknized differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77ace7-387d-45ae-86eb-f55d43115a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ensemble approach: if 2/ 3 models agree, then do 2/3 models. Else, don't.\n",
    "from tqdm import tqdm\n",
    "def ensemble_approach(SVM_result_dict, T5_result_dict, BERT_result_dict):\n",
    "    print(SVM_result_dict.keys())\n",
    "    print(T5_result_dict.keys())\n",
    "    print(BERT_result_dict.keys())\n",
    "    \n",
    "    print(len(SVM_result_dict['predictions']))\n",
    "    print(len(T5_result_dict['predictions']))\n",
    "    print(len(BERT_result_dict['predictions']))\n",
    "    majority_agreement_predictions = []\n",
    "    for i in tqdm(range(len(SVM_result_dict['ground_truth']))):\n",
    "            T5_result = 0 \n",
    "            BERT_result =  0\n",
    "            SVM_result = SVM_result_dict['predictions'][i]\n",
    "            ground_truth = SVM_result_dict['ground_truth'][i]\n",
    "            sentence = ''\n",
    "            for j in range(len((T5_result_dict['row_numbers']))):\n",
    "                if T5_result_dict['row_numbers'][j] == i:\n",
    "                    sentence = T5_result_dict['decoded_sentences'][j]\n",
    "                    #this means that T5[j] == SVM[i]\n",
    "                    T5_result = T5_result_dict['predictions'][j]\n",
    "            for k in range(len((BERT_result_dict['row_numbers']))):\n",
    "                if BERT_result_dict['row_numbers'][k] == i:\n",
    "                    #this means that BERT[k] == SVM[i]\n",
    "                    BERT_result = BERT_result_dict['predictions'][k]\n",
    "                    \n",
    "            #If 2/3 agree\n",
    "            #print('sentence: ', sentence)\n",
    "            if ( BERT_result == 1 or SVM_result == 1 or T5_result == 1):\n",
    "                majority_agreement_predictions.append(1)\n",
    "            else:\n",
    "                majority_agreement_predictions.append(0)\n",
    "                \n",
    "    return(majority_agreement_predictions)\n",
    "\n",
    "majority_agreement_predictions = ensemble_approach(SVM_result_dict, T5_result_dict, BERT_result_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd6b80-ffdb-4f17-bf3a-55d3d2ba91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "classification_report_result = classification_report(SVM_result_dict['ground_truth'], majority_agreement_predictions, digits=3)\n",
    "print(classification_report_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
